{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from functools import reduce\n",
    "import random\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2516ea65b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC():\n",
    "    def __init__(self, D_in, D_out):\n",
    "        self.cache = None\n",
    "        self.W = {'val': np.random.normal(0.0, np.sqrt(2/D_in), (D_in,D_out)), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(D_out), 'grad': 0}\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = np.dot(X, self.W['val']) + self.b['val']\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        X = self.cache\n",
    "        dX = np.dot(dout, self.W['val'].T).reshape(X.shape)\n",
    "        self.W['grad'] = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, dout)\n",
    "        self.b['grad'] = np.sum(dout, axis=0)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv():\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weight_size = (self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        self.stride = stride\n",
    "\n",
    "        self.W = {'val': np.random.standard_normal(self.weight_size), 'grad': 0}\n",
    "        self.b = {'val':  np.random.standard_normal((self.out_channels,1)), 'grad': 0}\n",
    "        \n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        (N,C,H,W) = x.shape\n",
    "        self.input_shape = x.shape\n",
    "        H_out = (H - self.kernel_size) // self.stride + 1\n",
    "        W_out = (W - self.kernel_size) // self.stride + 1\n",
    "        conv_out = np.zeros((N, self.out_channels, H_out, W_out))\n",
    "        self.col_image = []\n",
    "        \n",
    "        weight_cols = self.W['val'].reshape(self.out_channels, -1)\n",
    "        self.col_image = im2col_fast(x,self.kernel_size,self.stride)\n",
    "        conv_out = (np.dot(self.col_image, weight_cols.T) + self.b['val'].T).transpose(0,2,1).reshape(N, self.out_channels, H_out, W_out)\n",
    "        \n",
    "        return conv_out\n",
    "        \n",
    "    def backward(self, error):\n",
    "        (N,C,_,_) = error.shape\n",
    "        error_col = error.reshape(N,C,-1)\n",
    "        for i in range(N):\n",
    "            self.W['grad'] += np.dot(error_col[i], self.col_image[i]).reshape(self.W['val'].shape)\n",
    "        self.b['grad'] += np.sum(error_col, axis=(0,2)).reshape(self.b['val'].shape)\n",
    "        \n",
    "        error_pad =np.pad(error, ((0,0), (0,0), (self.kernel_size - 1, self.kernel_size - 1),\n",
    "                          (self.kernel_size - 1, self.kernel_size - 1)), 'constant', constant_values=0)\n",
    "        \n",
    "        flip_weights = self.W['val'][:, :, ::-1, ::-1]\n",
    "        flip_weights = flip_weights.swapaxes(0,1) \n",
    "        col_flip_weights = flip_weights.reshape(self.in_channels, -1)\n",
    "        \n",
    "        col_pad_delta = im2col_fast(error_pad,self.kernel_size,self.stride)\n",
    "        next_delta = np.dot(col_pad_delta, col_flip_weights.T)\n",
    "        next_delta = np.reshape(next_delta.transpose(0,2,1), self.input_shape)\n",
    "        \n",
    "        return next_delta\n",
    "    \n",
    "    \n",
    "def im2col_fast(image, kernel_size, stride):\n",
    "    N, C, H, W = image.shape\n",
    "    H_out = (H - kernel_size) // stride + 1\n",
    "    W_out = (W - kernel_size) // stride + 1\n",
    "    shape = (N, C, H_out, W_out, kernel_size, kernel_size)\n",
    "    strides = (*image.strides[:-2], image.strides[-2]*stride, image.strides[-1]*stride, *image.strides[-2:])\n",
    "    A = np.lib.stride_tricks.as_strided(image, shape=shape, strides=strides)\n",
    "    return A.transpose(0,2,3,1,4,5).reshape(N, H_out*W_out, C*kernel_size*kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, pool_h, pool_w, strides):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.strides = strides\n",
    "        self.cache = None\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        assert self.pool_h == self.pool_w == self.strides, 'Invalid pool params'\n",
    "        assert H % self.pool_h == 0\n",
    "        assert W % self.pool_w == 0\n",
    "        x_reshaped = x.reshape(N, C, H // self.pool_h, self.pool_h,\n",
    "                               W // self.pool_w, self.pool_w)\n",
    "        out = x_reshaped.max(axis=3).max(axis=4)\n",
    "\n",
    "        self.cache = (x, x_reshaped, out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): \n",
    "        dx_reshaped = np.zeros_like(self.cache[1])\n",
    "        out_newaxis = self.cache[2][:, :, :, np.newaxis, :, np.newaxis]\n",
    "        mask = (self.cache[1] == out_newaxis)\n",
    "        dout_newaxis = dout[:, :, :, np.newaxis, :, np.newaxis]\n",
    "        dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, dx_reshaped)\n",
    "        dx_reshaped = dout_broadcast * mask\n",
    "        #The line blow is to ensure everyone get correct result\n",
    "        #dx_reshaped /= np.sum(mask, axis=(3, 5), keepdims=True)\n",
    "        dx = dx_reshaped.reshape(self.cache[0].shape)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Softmax activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build Softmax\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"Softmax: _forward\")\n",
    "        maxes = np.amax(X, axis=1)\n",
    "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "        Y = np.exp(X - maxes)\n",
    "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "        self.cache = (X, Y, Z)\n",
    "        return Z # distribution\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, Y, Z = self.cache\n",
    "        dZ = np.zeros(X.shape)\n",
    "        dY = np.zeros(X.shape)\n",
    "        dX = np.zeros(X.shape)\n",
    "        N = X.shape[0]\n",
    "        for n in range(N):\n",
    "            i = np.argmax(Z[n])\n",
    "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
    "            M = np.zeros((N,N))\n",
    "            M[:,i] = 1\n",
    "            dY[n,:] = np.eye(N) - M\n",
    "        dX = np.dot(dout,dZ)\n",
    "        dX = np.dot(dX,dY)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLLoss(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Negative log likelihood loss\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N = Y_pred.shape[0]\n",
    "    M = np.sum(Y_pred*Y_true, axis=1)\n",
    "    for e in M:\n",
    "        #print(e)\n",
    "        if e == 0:\n",
    "            loss += 500\n",
    "        else:\n",
    "            loss += -np.log(e)\n",
    "    return loss/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        softmax = Softmax()\n",
    "        prob = softmax._forward(Y_pred)\n",
    "        loss = NLLLoss(prob, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = prob.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build ReLU\")\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(\"ReLU: _forward\")\n",
    "        out = np.maximum(0, X)\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #print(\"ReLU: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.array(dout, copy=True)\n",
    "        dX[X <= 0] = 0\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    \"\"\"\n",
    "    Dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p=1):\n",
    "        self.cache = None\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, X):\n",
    "        random_tensor = np.random.binomial(n=1, p=self.p, size=X.shape)\n",
    "        self.cache = X, random_tensor\n",
    "        return X*random_tensor\n",
    "\n",
    "    def backward(self, dout):\n",
    "        X, random_tensor = self.cache\n",
    "        dX = dout*random_tensor/self.p\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, params, lr=0.001, reg=0):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param['val'] -= (self.lr*param['grad'] + self.reg*param['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum():\n",
    "    def __init__(self, params, lr=0.001, momentum=0.99, reg=0):\n",
    "        self.l = len(params)\n",
    "        self.parameters = params\n",
    "        self.velocities = []\n",
    "        for param in self.parameters:\n",
    "            self.velocities.append(np.zeros(param['val'].shape))\n",
    "        self.lr = lr\n",
    "        self.rho = momentum\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for i in range(self.l):\n",
    "            self.velocities[i] = self.rho*self.velocities[i] + (1-self.rho)*self.parameters[i]['grad']\n",
    "            self.parameters[i]['val'] -= (self.lr*self.velocities[i] + self.reg*self.parameters[i]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5():\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv(1, 6, 5)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.pool1 = MaxPool(2, 2, 2)\n",
    "        self.conv2 = Conv(6, 16, 5)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.pool2 = MaxPool(2, 2, 2)\n",
    "        self.FC1 = FC(16*4*4, 120)\n",
    "        self.ReLU3 = ReLU()\n",
    "        self.FC2 = FC(120, 84)\n",
    "        self.ReLU4 = ReLU()\n",
    "        self.FC3 = FC(84, 10)\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "        self.p2_shape = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1.forward(X)\n",
    "        a1 = self.ReLU1.forward(h1)\n",
    "        p1 = self.pool1.forward(a1)\n",
    "        h2 = self.conv2.forward(p1)\n",
    "        a2 = self.ReLU2.forward(h2)\n",
    "        p2 = self.pool2.forward(a2)\n",
    "        self.p2_shape = p2.shape\n",
    "        fl = p2.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1.forward(fl)\n",
    "        a3 = self.ReLU3.forward(h3)\n",
    "        h4 = self.FC2.forward(a3)\n",
    "        a5 = self.ReLU4.forward(h4)\n",
    "        a5 = self.dropout.forward(a5)\n",
    "        h5 = self.FC3.forward(a5)\n",
    "        return h5\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.FC3.backward(dout)\n",
    "        dout = self.dropout.backward(dout)\n",
    "        dout = self.ReLU4.backward(dout)\n",
    "        dout = self.FC2.backward(dout)\n",
    "        dout = self.ReLU3.backward(dout)\n",
    "        dout = self.FC1.backward(dout)\n",
    "        dout = dout.reshape(self.p2_shape) # reshape\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.ReLU2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.ReLU1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeOneHot(Y, D_out):\n",
    "    N = Y.shape[0]\n",
    "    Z = np.zeros((N, D_out))\n",
    "    Z[np.arange(N), Y] = 1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_losses(losses):\n",
    "    t = np.arange(len(losses))\n",
    "    plt.plot(t, losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, batch_size):\n",
    "    N = len(X)\n",
    "    i = random.randint(1, N-batch_size)\n",
    "    return X[i:i+batch_size], Y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load()\n",
    "X_train, X_test = X_train/float(255), X_test/float(255)\n",
    "X_train -= np.mean(X_train)\n",
    "X_test -= np.mean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "D_in = 784\n",
    "D_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% iter: 0, loss: 26.766335686382345\n",
      "0.3333333333333333% iter: 100, loss: 1.2647777989830407\n",
      "0.6666666666666666% iter: 200, loss: 1.1379637346683427\n",
      "1.0% iter: 300, loss: 1.0388306969999805\n",
      "1.3333333333333333% iter: 400, loss: 1.028863427739907\n",
      "1.6666666666666667% iter: 500, loss: 0.8546929275056371\n",
      "2.0% iter: 600, loss: 0.44176049027208236\n",
      "2.3333333333333335% iter: 700, loss: 1.0402891007713824\n",
      "2.6666666666666665% iter: 800, loss: 0.5013313740926478\n",
      "3.0% iter: 900, loss: 0.48464555878017\n",
      "3.3333333333333335% iter: 1000, loss: 1.0708026162989805\n",
      "3.6666666666666665% iter: 1100, loss: 0.165039697729086\n",
      "4.0% iter: 1200, loss: 0.42445265332292426\n",
      "4.333333333333333% iter: 1300, loss: 0.5016624386634733\n",
      "4.666666666666667% iter: 1400, loss: 0.5106261734581738\n",
      "5.0% iter: 1500, loss: 0.28806392600641617\n",
      "5.333333333333333% iter: 1600, loss: 0.4282393511024754\n",
      "5.666666666666667% iter: 1700, loss: 0.538116785899257\n",
      "6.0% iter: 1800, loss: 0.6338092073561521\n",
      "6.333333333333333% iter: 1900, loss: 0.34638671429805495\n",
      "6.666666666666667% iter: 2000, loss: 0.6627080841544154\n",
      "7.0% iter: 2100, loss: 0.5090512090271568\n",
      "7.333333333333333% iter: 2200, loss: 0.2799562545330636\n",
      "7.666666666666667% iter: 2300, loss: 0.2680596651130184\n",
      "8.0% iter: 2400, loss: 0.4141845255425471\n",
      "8.333333333333334% iter: 2500, loss: 0.30417782557579426\n",
      "8.666666666666666% iter: 2600, loss: 0.26558189445085456\n",
      "9.0% iter: 2700, loss: 0.5111395579735107\n",
      "9.333333333333334% iter: 2800, loss: 0.4164476783816035\n",
      "9.666666666666666% iter: 2900, loss: 0.41790700609577686\n",
      "10.0% iter: 3000, loss: 0.14150420322722618\n",
      "10.333333333333334% iter: 3100, loss: 0.2658818277819046\n",
      "10.666666666666666% iter: 3200, loss: 0.612298214548671\n",
      "11.0% iter: 3300, loss: 0.16198106886817418\n",
      "11.333333333333334% iter: 3400, loss: 0.4055943895648136\n",
      "11.666666666666666% iter: 3500, loss: 0.23638801643899632\n",
      "12.0% iter: 3600, loss: 0.2853204338177715\n",
      "12.333333333333334% iter: 3700, loss: 0.17041788240516642\n",
      "12.666666666666666% iter: 3800, loss: 0.04066567613448538\n",
      "13.0% iter: 3900, loss: 0.11784662567640539\n",
      "13.333333333333334% iter: 4000, loss: 0.320907562185567\n",
      "13.666666666666666% iter: 4100, loss: 0.1922370940856494\n",
      "14.0% iter: 4200, loss: 0.11249392485303675\n",
      "14.333333333333334% iter: 4300, loss: 0.5221082765366882\n",
      "14.666666666666666% iter: 4400, loss: 0.2698094728563022\n",
      "15.0% iter: 4500, loss: 0.28275277913636\n",
      "15.333333333333334% iter: 4600, loss: 0.4367852907760998\n",
      "15.666666666666666% iter: 4700, loss: 0.15359324161872162\n",
      "16.0% iter: 4800, loss: 0.7568807840112378\n",
      "16.333333333333332% iter: 4900, loss: 0.4411218780729424\n",
      "16.666666666666668% iter: 5000, loss: 0.1586298779287595\n",
      "17.0% iter: 5100, loss: 0.23237544203647592\n",
      "17.333333333333332% iter: 5200, loss: 0.1449417311950752\n",
      "17.666666666666668% iter: 5300, loss: 0.20791856919699547\n",
      "18.0% iter: 5400, loss: 0.09537172063801183\n",
      "18.333333333333332% iter: 5500, loss: 0.4806285224855471\n",
      "18.666666666666668% iter: 5600, loss: 0.5261907847146969\n",
      "19.0% iter: 5700, loss: 0.21237113942580152\n",
      "19.333333333333332% iter: 5800, loss: 0.2830134340591331\n",
      "19.666666666666668% iter: 5900, loss: 0.11331956546066287\n",
      "20.0% iter: 6000, loss: 0.4008400475047041\n",
      "20.333333333333332% iter: 6100, loss: 0.17683259205352708\n",
      "20.666666666666668% iter: 6200, loss: 0.22796375792639686\n",
      "21.0% iter: 6300, loss: 0.2320515153228359\n",
      "21.333333333333332% iter: 6400, loss: 0.08602717952162366\n",
      "21.666666666666668% iter: 6500, loss: 0.37830370162574356\n",
      "22.0% iter: 6600, loss: 0.5579498310415892\n",
      "22.333333333333332% iter: 6700, loss: 0.15931910421555984\n",
      "22.666666666666668% iter: 6800, loss: 0.1306549794372876\n",
      "23.0% iter: 6900, loss: 0.4865169200017931\n",
      "23.333333333333332% iter: 7000, loss: 0.10617735839356676\n",
      "23.666666666666668% iter: 7100, loss: 0.10236901841161461\n",
      "24.0% iter: 7200, loss: 0.04499570450771901\n",
      "24.333333333333332% iter: 7300, loss: 0.20402425771751898\n",
      "24.666666666666668% iter: 7400, loss: 0.34236154760653154\n",
      "25.0% iter: 7500, loss: 0.2760586505619084\n",
      "25.333333333333332% iter: 7600, loss: 0.4042479266069017\n",
      "25.666666666666668% iter: 7700, loss: 0.2426977369448109\n",
      "26.0% iter: 7800, loss: 0.26733842042307787\n",
      "26.333333333333332% iter: 7900, loss: 0.12247354525587978\n",
      "26.666666666666668% iter: 8000, loss: 0.3657303761042377\n",
      "27.0% iter: 8100, loss: 0.14667232924128573\n",
      "27.333333333333332% iter: 8200, loss: 0.18304912782694788\n",
      "27.666666666666668% iter: 8300, loss: 0.31028563139668486\n",
      "28.0% iter: 8400, loss: 0.7958598696095679\n",
      "28.333333333333332% iter: 8500, loss: 0.2304631482508855\n",
      "28.666666666666668% iter: 8600, loss: 0.07320458474826548\n",
      "29.0% iter: 8700, loss: 0.09715025924814097\n",
      "29.333333333333332% iter: 8800, loss: 0.1765181724629606\n",
      "29.666666666666668% iter: 8900, loss: 0.08262689282184224\n",
      "30.0% iter: 9000, loss: 0.06480511392050758\n",
      "30.333333333333332% iter: 9100, loss: 0.14301425576164853\n",
      "30.666666666666668% iter: 9200, loss: 0.22959402695086495\n",
      "31.0% iter: 9300, loss: 0.10356006201336464\n",
      "31.333333333333332% iter: 9400, loss: 0.23267040703173003\n",
      "31.666666666666668% iter: 9500, loss: 0.08718317482827531\n",
      "32.0% iter: 9600, loss: 0.34364950100355807\n",
      "32.333333333333336% iter: 9700, loss: 0.14248529656539685\n",
      "32.666666666666664% iter: 9800, loss: 0.41011058271115086\n",
      "33.0% iter: 9900, loss: 0.06247598877648136\n",
      "33.333333333333336% iter: 10000, loss: 0.14763207635422965\n",
      "33.666666666666664% iter: 10100, loss: 0.21494636535999417\n",
      "34.0% iter: 10200, loss: 0.11092516731261813\n",
      "34.333333333333336% iter: 10300, loss: 0.06118375854853877\n",
      "34.666666666666664% iter: 10400, loss: 0.08668740920752552\n",
      "35.0% iter: 10500, loss: 0.01343801386409363\n",
      "35.333333333333336% iter: 10600, loss: 0.07988826447655703\n",
      "35.666666666666664% iter: 10700, loss: 0.3843003903968371\n",
      "36.0% iter: 10800, loss: 0.5524456537444699\n",
      "36.333333333333336% iter: 10900, loss: 0.12476179883242741\n",
      "36.666666666666664% iter: 11000, loss: 0.07074913293959675\n",
      "37.0% iter: 11100, loss: 0.08357334317382566\n",
      "37.333333333333336% iter: 11200, loss: 0.1260850085833502\n",
      "37.666666666666664% iter: 11300, loss: 0.1706978288306415\n",
      "38.0% iter: 11400, loss: 0.1691829873743864\n",
      "38.333333333333336% iter: 11500, loss: 0.15261894552764277\n",
      "38.666666666666664% iter: 11600, loss: 0.022167914391587478\n",
      "39.0% iter: 11700, loss: 0.0688001829294405\n",
      "39.333333333333336% iter: 11800, loss: 0.1154418588590574\n",
      "39.666666666666664% iter: 11900, loss: 0.05699555607390986\n",
      "40.0% iter: 12000, loss: 0.17093440128392054\n",
      "40.333333333333336% iter: 12100, loss: 0.14267543486547807\n",
      "40.666666666666664% iter: 12200, loss: 0.2922593598431375\n",
      "41.0% iter: 12300, loss: 0.12033128927616094\n",
      "41.333333333333336% iter: 12400, loss: 0.0439364008231506\n",
      "41.666666666666664% iter: 12500, loss: 0.20928807592850204\n",
      "42.0% iter: 12600, loss: 0.03759136416018534\n",
      "42.333333333333336% iter: 12700, loss: 0.07763397744619464\n",
      "42.666666666666664% iter: 12800, loss: 0.22025014383675762\n",
      "43.0% iter: 12900, loss: 0.22157573052952612\n",
      "43.333333333333336% iter: 13000, loss: 0.01756465218364267\n",
      "43.666666666666664% iter: 13100, loss: 0.053602412931249246\n",
      "44.0% iter: 13200, loss: 0.11108397830445635\n",
      "44.333333333333336% iter: 13300, loss: 0.13732327277374617\n",
      "44.666666666666664% iter: 13400, loss: 0.09154428695141205\n",
      "45.0% iter: 13500, loss: 0.10004181286959513\n",
      "45.333333333333336% iter: 13600, loss: 0.1427802696616217\n",
      "45.666666666666664% iter: 13700, loss: 0.12569757048064395\n",
      "46.0% iter: 13800, loss: 0.08563406507413017\n",
      "46.333333333333336% iter: 13900, loss: 0.0871012283225511\n",
      "46.666666666666664% iter: 14000, loss: 0.09050109225087223\n",
      "47.0% iter: 14100, loss: 0.09273277251015488\n",
      "47.333333333333336% iter: 14200, loss: 0.04161236429362969\n",
      "47.666666666666664% iter: 14300, loss: 0.21975263041273776\n",
      "48.0% iter: 14400, loss: 0.20891234394765726\n",
      "48.333333333333336% iter: 14500, loss: 0.06877435169732966\n",
      "48.666666666666664% iter: 14600, loss: 0.07291447179266528\n",
      "49.0% iter: 14700, loss: 0.08985525972045903\n",
      "49.333333333333336% iter: 14800, loss: 0.13764468468355398\n",
      "49.666666666666664% iter: 14900, loss: 0.029832290514567553\n",
      "50.0% iter: 15000, loss: 0.09917948273293933\n",
      "50.333333333333336% iter: 15100, loss: 0.03208402435941547\n",
      "50.666666666666664% iter: 15200, loss: 0.15456433552656956\n",
      "51.0% iter: 15300, loss: 0.26754751376042835\n",
      "51.333333333333336% iter: 15400, loss: 0.0607894519493052\n",
      "51.666666666666664% iter: 15500, loss: 0.09871902946874464\n",
      "52.0% iter: 15600, loss: 0.06313122194047209\n",
      "52.333333333333336% iter: 15700, loss: 0.14696502970719713\n",
      "52.666666666666664% iter: 15800, loss: 0.07003654760757731\n",
      "53.0% iter: 15900, loss: 0.34350722840510906\n",
      "53.333333333333336% iter: 16000, loss: 0.08193148640976805\n",
      "53.666666666666664% iter: 16100, loss: 0.1293981743075056\n",
      "54.0% iter: 16200, loss: 0.18109250474574246\n",
      "54.333333333333336% iter: 16300, loss: 0.3843065586636387\n",
      "54.666666666666664% iter: 16400, loss: 0.1274319839970802\n",
      "55.0% iter: 16500, loss: 0.08191957210327318\n",
      "55.333333333333336% iter: 16600, loss: 0.06339038860267604\n",
      "55.666666666666664% iter: 16700, loss: 0.01231234456770177\n",
      "56.0% iter: 16800, loss: 0.0357331785848633\n",
      "56.333333333333336% iter: 16900, loss: 0.16249205806574007\n",
      "56.666666666666664% iter: 17000, loss: 0.01409713450744729\n",
      "57.0% iter: 17100, loss: 0.06719233038460085\n",
      "57.333333333333336% iter: 17200, loss: 0.03735123558437873\n",
      "57.666666666666664% iter: 17300, loss: 0.016651563385685755\n",
      "58.0% iter: 17400, loss: 0.05733954199913309\n",
      "58.333333333333336% iter: 17500, loss: 0.1055485381728459\n",
      "58.666666666666664% iter: 17600, loss: 0.019509001326279845\n",
      "59.0% iter: 17700, loss: 0.10018212642091386\n",
      "59.333333333333336% iter: 17800, loss: 0.059427679627140684\n",
      "59.666666666666664% iter: 17900, loss: 0.04161074073037315\n",
      "60.0% iter: 18000, loss: 0.035340490467121906\n",
      "60.333333333333336% iter: 18100, loss: 0.03200612476701425\n",
      "60.666666666666664% iter: 18200, loss: 0.14755442435460125\n",
      "61.0% iter: 18300, loss: 0.055278201959827335\n",
      "61.333333333333336% iter: 18400, loss: 0.01045307495607304\n",
      "61.666666666666664% iter: 18500, loss: 0.01521678090346957\n",
      "62.0% iter: 18600, loss: 0.028056619363457092\n",
      "62.333333333333336% iter: 18700, loss: 0.05308332022698718\n",
      "62.666666666666664% iter: 18800, loss: 0.1423639485744608\n",
      "63.0% iter: 18900, loss: 0.03878658831088933\n",
      "63.333333333333336% iter: 19000, loss: 0.0454396994679006\n",
      "63.666666666666664% iter: 19100, loss: 0.08796871119072602\n",
      "64.0% iter: 19200, loss: 0.07750662918520625\n",
      "64.33333333333333% iter: 19300, loss: 0.0873146637795845\n",
      "64.66666666666667% iter: 19400, loss: 0.022564517567370674\n",
      "65.0% iter: 19500, loss: 0.024372490144611722\n",
      "65.33333333333333% iter: 19600, loss: 0.19566310263523304\n",
      "65.66666666666667% iter: 19700, loss: 0.253274195500038\n",
      "66.0% iter: 19800, loss: 0.13759831900522723\n",
      "66.33333333333333% iter: 19900, loss: 0.2049296799731912\n",
      "66.66666666666667% iter: 20000, loss: 0.06870353789606709\n",
      "67.0% iter: 20100, loss: 0.05251093868384694\n",
      "67.33333333333333% iter: 20200, loss: 0.031558400189251504\n",
      "67.66666666666667% iter: 20300, loss: 0.18985274676870637\n",
      "68.0% iter: 20400, loss: 0.0017751911724118398\n",
      "68.33333333333333% iter: 20500, loss: 0.08087195461594321\n",
      "68.66666666666667% iter: 20600, loss: 0.14747571333042472\n",
      "69.0% iter: 20700, loss: 0.05229251658797296\n",
      "69.33333333333333% iter: 20800, loss: 0.10949755047148087\n",
      "69.66666666666667% iter: 20900, loss: 0.135195132334825\n",
      "70.0% iter: 21000, loss: 0.024711666186685428\n",
      "70.33333333333333% iter: 21100, loss: 0.08228611587352588\n",
      "70.66666666666667% iter: 21200, loss: 0.10920923629472894\n",
      "71.0% iter: 21300, loss: 0.13996341953709088\n",
      "71.33333333333333% iter: 21400, loss: 0.0657049587450723\n",
      "71.66666666666667% iter: 21500, loss: 0.12507861630918662\n",
      "72.0% iter: 21600, loss: 0.032496277764601976\n",
      "72.33333333333333% iter: 21700, loss: 0.003508367082300326\n",
      "72.66666666666667% iter: 21800, loss: 0.08303262806436529\n",
      "73.0% iter: 21900, loss: 0.08146752258262592\n",
      "73.33333333333333% iter: 22000, loss: 0.030654190992925278\n",
      "73.66666666666667% iter: 22100, loss: 0.062072132237081815\n",
      "74.0% iter: 22200, loss: 0.1172013273988485\n",
      "74.33333333333333% iter: 22300, loss: 0.03157070305475955\n",
      "74.66666666666667% iter: 22400, loss: 0.04096531861669554\n",
      "75.0% iter: 22500, loss: 0.05192307895538984\n",
      "75.33333333333333% iter: 22600, loss: 0.04867457333699143\n",
      "75.66666666666667% iter: 22700, loss: 0.09804481341641387\n",
      "76.0% iter: 22800, loss: 0.04373690075148266\n",
      "76.33333333333333% iter: 22900, loss: 0.06241560459761596\n",
      "76.66666666666667% iter: 23000, loss: 0.09217729513118007\n",
      "77.0% iter: 23100, loss: 0.10137406037736797\n",
      "77.33333333333333% iter: 23200, loss: 0.06789088582449716\n",
      "77.66666666666667% iter: 23300, loss: 0.13371392439326346\n",
      "78.0% iter: 23400, loss: 0.23215038918166322\n",
      "78.33333333333333% iter: 23500, loss: 0.01359197519843585\n",
      "78.66666666666667% iter: 23600, loss: 0.04323053581885302\n",
      "79.0% iter: 23700, loss: 0.06920131990866032\n",
      "79.33333333333333% iter: 23800, loss: 0.08881749256805892\n",
      "79.66666666666667% iter: 23900, loss: 0.09259800531507387\n",
      "80.0% iter: 24000, loss: 0.024660833787449772\n",
      "80.33333333333333% iter: 24100, loss: 0.11860166420642955\n",
      "80.66666666666667% iter: 24200, loss: 0.21151524515131112\n",
      "81.0% iter: 24300, loss: 0.1114260833930231\n",
      "81.33333333333333% iter: 24400, loss: 0.10405000121069552\n",
      "81.66666666666667% iter: 24500, loss: 0.033061326194995445\n",
      "82.0% iter: 24600, loss: 0.04517812051558915\n",
      "82.33333333333333% iter: 24700, loss: 0.04371501266615405\n",
      "82.66666666666667% iter: 24800, loss: 0.1210554931740902\n",
      "83.0% iter: 24900, loss: 0.20561754575101146\n",
      "83.33333333333333% iter: 25000, loss: 0.006499300410731466\n",
      "83.66666666666667% iter: 25100, loss: 0.0386176955896482\n",
      "84.0% iter: 25200, loss: 0.02465170781495588\n",
      "84.33333333333333% iter: 25300, loss: 0.04002059207184569\n",
      "84.66666666666667% iter: 25400, loss: 0.03714579711256753\n",
      "85.0% iter: 25500, loss: 0.10804501834702009\n",
      "85.33333333333333% iter: 25600, loss: 0.09497768031122074\n",
      "85.66666666666667% iter: 25700, loss: 0.06807250706385125\n",
      "86.0% iter: 25800, loss: 0.3308921037142956\n",
      "86.33333333333333% iter: 25900, loss: 0.054721658584174195\n",
      "86.66666666666667% iter: 26000, loss: 0.22566841959672385\n",
      "87.0% iter: 26100, loss: 0.11581449774030063\n",
      "87.33333333333333% iter: 26200, loss: 0.08791421001959437\n",
      "87.66666666666667% iter: 26300, loss: 0.03573280102735954\n",
      "88.0% iter: 26400, loss: 0.012284968596513339\n",
      "88.33333333333333% iter: 26500, loss: 0.04751273522522518\n",
      "88.66666666666667% iter: 26600, loss: 0.1729167854141908\n",
      "89.0% iter: 26700, loss: 0.0705877234084899\n",
      "89.33333333333333% iter: 26800, loss: 0.10855655066503934\n",
      "89.66666666666667% iter: 26900, loss: 0.008732737263888625\n",
      "90.0% iter: 27000, loss: 0.07504523044211317\n",
      "90.33333333333333% iter: 27100, loss: 0.10853752528463394\n",
      "90.66666666666667% iter: 27200, loss: 0.061507137728899874\n",
      "91.0% iter: 27300, loss: 0.134050354602358\n",
      "91.33333333333333% iter: 27400, loss: 0.03183556119354466\n",
      "91.66666666666667% iter: 27500, loss: 0.08406872183856014\n",
      "92.0% iter: 27600, loss: 0.03415092671513013\n",
      "92.33333333333333% iter: 27700, loss: 0.05617311358278478\n",
      "92.66666666666667% iter: 27800, loss: 0.09318214557944085\n",
      "93.0% iter: 27900, loss: 0.0336714684047375\n",
      "93.33333333333333% iter: 28000, loss: 0.13340122212046954\n",
      "93.66666666666667% iter: 28100, loss: 0.06272257086358604\n",
      "94.0% iter: 28200, loss: 0.13234274706480512\n",
      "94.33333333333333% iter: 28300, loss: 0.09334682556791145\n",
      "94.66666666666667% iter: 28400, loss: 0.028639615651168102\n",
      "95.0% iter: 28500, loss: 0.0933253219443102\n",
      "95.33333333333333% iter: 28600, loss: 0.1738546127539734\n",
      "95.66666666666667% iter: 28700, loss: 0.13284609996214375\n",
      "96.0% iter: 28800, loss: 0.0643970513716124\n",
      "96.33333333333333% iter: 28900, loss: 0.0966460948748373\n",
      "96.66666666666667% iter: 29000, loss: 0.08992517207076194\n",
      "97.0% iter: 29100, loss: 0.07889566298089931\n",
      "97.33333333333333% iter: 29200, loss: 0.07844834582598002\n",
      "97.66666666666667% iter: 29300, loss: 0.17273588634597659\n",
      "98.0% iter: 29400, loss: 0.06300354076685064\n",
      "98.33333333333333% iter: 29500, loss: 0.04936977899853979\n",
      "98.66666666666667% iter: 29600, loss: 0.046495654465423926\n",
      "99.0% iter: 29700, loss: 0.031273550587281464\n",
      "99.33333333333333% iter: 29800, loss: 0.06337543895677843\n",
      "99.66666666666667% iter: 29900, loss: 0.05298016364892289\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "losses = []\n",
    "optim = SGDMomentum(model.get_params(), lr=0.0001, momentum=0.80, reg=0.00003)\n",
    "criterion = CrossEntropyLoss()\n",
    "ITER = 30000\n",
    "for i in range(ITER):\n",
    "    # get batch, make onehot\n",
    "    X_batch, Y_batch = get_batch(X_train, Y_train, batch_size)\n",
    "    Y_batch = MakeOneHot(Y_batch, D_out)\n",
    "\n",
    "    # forward, loss, backward, step\n",
    "    X_batch = X_batch.reshape((batch_size, 1, 28, 28))\n",
    "    Y_pred = model.forward(X_batch)\n",
    "    loss, dout = criterion.get(Y_pred, Y_batch)\n",
    "    model.backward(dout)\n",
    "    optim.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"%s%% iter: %s, loss: %s\" % (100*i/ITER,i, loss))\n",
    "        losses.append(loss)\n",
    "        \n",
    "    if i == 10000:    \n",
    "        optim = SGDMomentum(model.get_params(), lr=0.00001, momentum=0.80, reg=0.00003)\n",
    "        \n",
    "    if i == 20000:    \n",
    "        optim = SGDMomentum(model.get_params(), lr=0.000001, momentum=0.80, reg=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save params\n",
    "weights = model.get_params()\n",
    "with open(\"weights.pkl\",\"wb\") as f:\n",
    "\tpickle.dump(weights, f)\n",
    "\n",
    "draw_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5():\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv(1, 6, 5)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.pool1 = MaxPool(2, 2, 2)\n",
    "        self.conv2 = Conv(6, 16, 5)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.pool2 = MaxPool(2, 2, 2)\n",
    "        self.FC1 = FC(16*4*4, 120)\n",
    "        self.ReLU3 = ReLU()\n",
    "        self.FC2 = FC(120, 84)\n",
    "        self.ReLU4 = ReLU()\n",
    "        self.FC3 = FC(84, 10)\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "        self.p2_shape = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1.forward(X)\n",
    "        a1 = self.ReLU1.forward(h1)\n",
    "        p1 = self.pool1.forward(a1)\n",
    "        h2 = self.conv2.forward(p1)\n",
    "        a2 = self.ReLU2.forward(h2)\n",
    "        p2 = self.pool2.forward(a2)\n",
    "        self.p2_shape = p2.shape\n",
    "        fl = p2.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1.forward(fl)\n",
    "        a3 = self.ReLU3.forward(h3)\n",
    "        h4 = self.FC2.forward(a3)\n",
    "        a5 = self.ReLU4.forward(h4)\n",
    "#         a5 = self.dropout.forward(a5)\n",
    "        h5 = self.FC3.forward(a5)\n",
    "        return h5\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.FC3.backward(dout)\n",
    "#         dout = self.dropout.backward(dout)\n",
    "        dout = self.ReLU4.backward(dout)\n",
    "        dout = self.FC2.backward(dout)\n",
    "        dout = self.ReLU3.backward(dout)\n",
    "        dout = self.FC1.backward(dout)\n",
    "        dout = dout.reshape(self.p2_shape) # reshape\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.ReLU2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.ReLU1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "with open(\"weights.pkl\", \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "model.set_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN--> Correct: 58880 out of 60000, acc=0.9813333333333333\n",
      "TEST--> Correct: 9795 out of 10000, acc=0.9795\n"
     ]
    }
   ],
   "source": [
    "# TRAIN SET ACC\n",
    "X_train = X_train.reshape((len(X_train),1,28,28))\n",
    "Y_pred = model.forward(X_train)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_train\n",
    "result = list(result)\n",
    "print(\"TRAIN--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_train.shape[0]) + \", acc=\" + str(result.count(0)/X_train.shape[0]))\n",
    "\n",
    "# TEST SET ACC\n",
    "X_test = X_test.reshape((len(X_test),1,28,28))\n",
    "Y_pred = model.forward(X_test)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_test\n",
    "result = list(result)\n",
    "print(\"TEST--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_test.shape[0]) + \", acc=\" + str(result.count(0)/X_test.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

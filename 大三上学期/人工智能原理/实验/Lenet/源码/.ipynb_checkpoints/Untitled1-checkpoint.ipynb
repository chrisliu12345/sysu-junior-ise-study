{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 16, D_in: 784, D_out: 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (64,25) and (150,16) not aligned: 25 (dim 1) != 150 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8b7d6e19097b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[1;31m# forward, loss, backward, step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-8b7d6e19097b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[0mp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m         \u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-8b7d6e19097b>\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mimg_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_image_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[0mconv_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_image_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_image_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (64,25) and (150,16) not aligned: 25 (dim 1) != 150 (dim 0)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "filename = [\n",
    "\t[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "\t[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "\t[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "\t[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "def MakeOneHot(Y, D_out):\n",
    "    N = Y.shape[0]\n",
    "    Z = np.zeros((N, D_out))\n",
    "    Z[np.arange(N), Y] = 1\n",
    "    return Z\n",
    "\n",
    "def draw_losses(losses):\n",
    "    t = np.arange(len(losses))\n",
    "    plt.plot(t, losses)\n",
    "    plt.show()\n",
    "\n",
    "def get_batch(X, Y, batch_size):\n",
    "    N = len(X)\n",
    "    i = random.randint(1, N-batch_size)\n",
    "    return X[i:i+batch_size], Y[i:i+batch_size]\n",
    "\n",
    "class FC():\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        #print(\"Build FC\")\n",
    "        self.cache = None\n",
    "        #self.W = {'val': np.random.randn(D_in, D_out), 'grad': 0}\n",
    "        self.W = {'val': np.random.normal(0.0, np.sqrt(2/D_in), (D_in,D_out)), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(D_out), 'grad': 0}\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"FC: _forward\")\n",
    "        out = np.dot(X, self.W['val']) + self.b['val']\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"FC: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.dot(dout, self.W['val'].T).reshape(X.shape)\n",
    "        self.W['grad'] = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, dout)\n",
    "        self.b['grad'] = np.sum(dout, axis=0)\n",
    "        #self._update_params()\n",
    "        return dX\n",
    "\n",
    "    def _update_params(self, lr=0.001):\n",
    "        # Update the parameters\n",
    "        self.W['val'] -= lr*self.W['grad']\n",
    "        self.b['val'] -= lr*self.b['grad']\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build ReLU\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"ReLU: _forward\")\n",
    "        out = np.maximum(0, X)\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"ReLU: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.array(dout, copy=True)\n",
    "        dX[X <= 0] = 0\n",
    "        return dX\n",
    "\n",
    "class Sigmoid():\n",
    "    \"\"\"\n",
    "    Sigmoid activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.cache = X\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X = self.cache\n",
    "        dX = dout*X*(1-X)\n",
    "        return dX\n",
    "\n",
    "class tanh():\n",
    "    \"\"\"\n",
    "    tanh activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = X\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.cache = X\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def _backward(self, X):\n",
    "        X = self.cache\n",
    "        dX = dout*(1 - np.tanh(X)**2)\n",
    "        return dX\n",
    "\n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Softmax activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build Softmax\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"Softmax: _forward\")\n",
    "        maxes = np.amax(X, axis=1)\n",
    "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "        Y = np.exp(X - maxes)\n",
    "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "        self.cache = (X, Y, Z)\n",
    "        return Z # distribution\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, Y, Z = self.cache\n",
    "        dZ = np.zeros(X.shape)\n",
    "        dY = np.zeros(X.shape)\n",
    "        dX = np.zeros(X.shape)\n",
    "        N = X.shape[0]\n",
    "        for n in range(N):\n",
    "            i = np.argmax(Z[n])\n",
    "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
    "            M = np.zeros((N,N))\n",
    "            M[:,i] = 1\n",
    "            dY[n,:] = np.eye(N) - M\n",
    "        dX = np.dot(dout,dZ)\n",
    "        dX = np.dot(dX,dY)\n",
    "        return dX\n",
    "\n",
    "class Dropout():\n",
    "    \"\"\"\n",
    "    Dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p=1):\n",
    "        self.cache = None\n",
    "        self.p = p\n",
    "\n",
    "    def _forward(self, X):\n",
    "        M = (np.random.rand(*X.shape) < self.p) / self.p\n",
    "        self.cache = X, M\n",
    "        return X*M\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, M = self.cache\n",
    "        dX = dout*M/self.p\n",
    "        return dX\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weight_size = (in_channels,out_channels,kernel_size,kernel_size)\n",
    "        self.stride = stride\n",
    "        self.cache = 0\n",
    "        \n",
    "        self.W = {'val': np.random.standard_normal(self.weight_size), 'grad': np.zeros(self.weight_size)}\n",
    "        self.b = {'val': np.random.standard_normal((out_channels,1)), 'grad': np.zeros((out_channels,1))}\n",
    "        \n",
    "    def _forward(self, x):\n",
    "        (N,Cin,H,W) = x.shape\n",
    "        self.input_shape = x.shape\n",
    "        H_ = int((H - self.kernel_size) / self.stride + 1)\n",
    "        W_ = int((W - self.kernel_size) / self.stride + 1)\n",
    "        col_weights = self.W['val'].transpose(1,0,2,3).reshape(self.out_channels,-1).T\n",
    "        self.col_image = []\n",
    "        conv_out = np.zeros((N,self.out_channels,H_,W_))\n",
    "        for i in range(N):\n",
    "            img_i = x[i, :]\n",
    "            self.col_image_i = self.im2col(img_i, self.kernel_size, self.stride)\n",
    "            conv_out[i] = np.reshape(np.dot(self.col_image_i, col_weights).T+self.b['val'], (self.out_channels, H_, W_))\n",
    "            self.col_image.append(self.col_image_i)\n",
    "        self.col_image = np.array(self.col_image)\n",
    "        return conv_out\n",
    "    \n",
    "    def _backward(self, dout):\n",
    "        (N,Cout,H_,W_) = dout.shape\n",
    "        col_dout = dout.reshape((N,Cout,-1))\n",
    "        \n",
    "        for i in range(self.input_shape[0]):\n",
    "            self.W['grad'] += np.dot(col_dout[i], self.col_image[i]).reshape(self.W['val'].shape)\n",
    "        self.b['grad'] += np.sum(col_dout,axis=(0,2)).reshape(self.b['val'].shape)\n",
    "        \n",
    "        dout_pad = np.pad(dout, ((0, 0), (0, 0), (self.kernel_size - 1, self.kernel_size - 1), (self.kernel_size - 1, self.kernel_size - 1)), 'constant', constant_values=0)\n",
    "        \n",
    "        weight_flip = self.W['val'][:,:,::-1,::-1]\n",
    "        weight_flip = np.swapaxes(weight_flip, 0, 1)\n",
    "        weight_flip_col = weight_flip.reshape(self.in_channels, -1)\n",
    "        \n",
    "        next_dout =  np.zeros((N, self.in_channels, self.input_shape[2], self.input_shape[3]))\n",
    "        for i in range(N):\n",
    "            dout_pad_image_batch_i = dout_pad[i,:]\n",
    "            dout_pad_image_batch_i_col = self.im2col(dout_pad_image_batch_i , self.kernel_size, self.stride)\n",
    "            next_dout[i] = np.reshape(np.dot(weight_flip_col, np.transpose(dout_pad_image_batch_i_col)), (self.in_channels, self.input_shape[2], self.input_shape[3]))\n",
    "        \n",
    "        return next_dout\n",
    "        \n",
    "    def im2col(self, image, ksize, stride):\n",
    "        # image is a 3d tensor([channel, height, width])\n",
    "        img_cols = None\n",
    "        for channel_i in range(image.shape[0]):\n",
    "            img_channel_i = image[channel_i,:]\n",
    "            img_channel_i_cols = []\n",
    "            for i in range(0, image.shape[1] - ksize + 1, stride):\n",
    "                for j in range(0, image.shape[2] - ksize + 1, stride):\n",
    "                    col = img_channel_i[i:i + ksize, j:j + ksize].reshape([-1])\n",
    "                    img_channel_i_cols.append(col)\n",
    "            img_channel_i_cols = np.array(img_channel_i_cols)\n",
    "        if img_cols is None:\n",
    "            img_cols = img_channel_i_cols\n",
    "        else:\n",
    "            img_cols = np.hstack((img_cols, img_channel_i_cols))\n",
    "        return img_cols\n",
    "\n",
    "class MaxPool():\n",
    "    def __init__(self, F, stride):\n",
    "        self.F = F\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        # X: (N, Cin, H, W): maxpool along 3rd, 4th dim\n",
    "        (N,Cin,H,W) = X.shape\n",
    "        F = self.F\n",
    "        W_ = W//F\n",
    "        H_ = H//F\n",
    "        Y = np.zeros((N,Cin,H_,W_))\n",
    "        M = np.zeros(X.shape) # mask\n",
    "        for n in range(N):\n",
    "            for cin in range(Cin):\n",
    "                for h_ in range(H_):\n",
    "                    for w_ in range(W_):\n",
    "                        Y[n,cin,h_,w_] = np.max(X[n,cin,F*h_:F*(h_+1),F*w_:F*(w_+1)])\n",
    "                        i,j = np.unravel_index(X[n,cin,F*h_:F*(h_+1),F*w_:F*(w_+1)].argmax(), (F,F))\n",
    "                        M[n,cin,F*h_+i,F*w_+j] = 1\n",
    "        self.cache = M\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        M = self.cache\n",
    "        (N,Cin,H,W) = M.shape\n",
    "        dout = np.array(dout)\n",
    "        #print(\"dout.shape: %s, M.shape: %s\" % (dout.shape, M.shape))\n",
    "        dX = np.zeros(M.shape)\n",
    "        for n in range(N):\n",
    "            for c in range(Cin):\n",
    "                #print(\"(n,c): (%s,%s)\" % (n,c))\n",
    "                dX[n,c,:,:] = dout[n,c,:,:].repeat(self.stride, axis=0).repeat(self.stride, axis=1)\n",
    "        return dX*M\n",
    "\n",
    "def NLLLoss(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Negative log likelihood loss\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N = Y_pred.shape[0]\n",
    "    M = np.sum(Y_pred*Y_true, axis=1)\n",
    "    for e in M:\n",
    "        #print(e)\n",
    "        if e == 0:\n",
    "            loss += 500\n",
    "        else:\n",
    "            loss += -np.log(e)\n",
    "    return loss/N\n",
    "\n",
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        softmax = Softmax()\n",
    "        prob = softmax._forward(Y_pred)\n",
    "        loss = NLLLoss(prob, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = prob.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout\n",
    "\n",
    "class SoftmaxLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        loss = NLLLoss(Y_pred, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = Y_pred.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout\n",
    "\n",
    "class Net(metaclass=ABCMeta):\n",
    "    # Neural network super class\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_params(self, params):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TwoLayerNet(Net):\n",
    "\n",
    "    #Simple 2 layer NN\n",
    "\n",
    "    def __init__(self, N, D_in, H, D_out, weights=''):\n",
    "        self.FC1 = FC(D_in, H)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.FC2 = FC(H, D_out)\n",
    "\n",
    "        if weights == '':\n",
    "            pass\n",
    "        else:\n",
    "            with open(weights,'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "                self.set_params(params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h1 = self.FC1._forward(X)\n",
    "        a1 = self.ReLU1._forward(h1)\n",
    "        h2 = self.FC2._forward(a1)\n",
    "        return h2\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.ReLU1._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b] = params\n",
    "\n",
    "\n",
    "class ThreeLayerNet(Net):\n",
    "\n",
    "    #Simple 3 layer NN\n",
    "\n",
    "    def __init__(self, N, D_in, H1, H2, D_out, weights=''):\n",
    "        self.FC1 = FC(D_in, H1)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.FC2 = FC(H1, H2)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.FC3 = FC(H2, D_out)\n",
    "\n",
    "        if weights == '':\n",
    "            pass\n",
    "        else:\n",
    "            with open(weights,'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "                self.set_params(params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h1 = self.FC1._forward(X)\n",
    "        a1 = self.ReLU1._forward(h1)\n",
    "        h2 = self.FC2._forward(a1)\n",
    "        a2 = self.ReLU2._forward(h2)\n",
    "        h3 = self.FC3._forward(a2)\n",
    "        return h3\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.ReLU2._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.ReLU1._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params\n",
    "\n",
    "\n",
    "class LeNet5(Net):\n",
    "    # LeNet5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv(1, 6, 5)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.pool1 = MaxPool(2,2)\n",
    "        self.conv2 = Conv(6, 16, 5)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.pool2 = MaxPool(2,2)\n",
    "        self.FC1 = FC(16*4*4, 120)\n",
    "        self.ReLU3 = ReLU()\n",
    "        self.FC2 = FC(120, 84)\n",
    "        self.ReLU4 = ReLU()\n",
    "        self.FC3 = FC(84, 10)\n",
    "        self.Softmax = Softmax()\n",
    "\n",
    "        self.p2_shape = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1._forward(X)\n",
    "        a1 = self.ReLU1._forward(h1)\n",
    "        p1 = self.pool1._forward(a1)\n",
    "        h2 = self.conv2._forward(p1)\n",
    "        a2 = self.ReLU2._forward(h2)\n",
    "        p2 = self.pool2._forward(a2)\n",
    "        self.p2_shape = p2.shape\n",
    "        fl = p2.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1._forward(fl)\n",
    "        a3 = self.ReLU3._forward(h3)\n",
    "        h4 = self.FC2._forward(a3)\n",
    "        a5 = self.ReLU4._forward(h4)\n",
    "        h5 = self.FC3._forward(a5)\n",
    "        # a5 = self.Softmax._forward(h5)\n",
    "        return h5\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout = self.Softmax._backward(dout)\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.ReLU4._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.ReLU3._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "        dout = dout.reshape(self.p2_shape) # reshape\n",
    "        dout = self.pool2._backward(dout)\n",
    "        dout = self.ReLU2._backward(dout)\n",
    "        dout = self.conv2._backward(dout)\n",
    "        dout = self.pool1._backward(dout)\n",
    "        dout = self.ReLU1._backward(dout)\n",
    "        dout = self.conv1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params, lr=0.001, reg=0):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param['val'] -= (self.lr*param['grad'] + self.reg*param['val'])\n",
    "\n",
    "class SGDMomentum():\n",
    "    def __init__(self, params, lr=0.001, momentum=0.99, reg=0):\n",
    "        self.l = len(params)\n",
    "        self.parameters = params\n",
    "        self.velocities = []\n",
    "        for param in self.parameters:\n",
    "            self.velocities.append(np.zeros(param['val'].shape))\n",
    "        self.lr = lr\n",
    "        self.rho = momentum\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for i in range(self.l):\n",
    "            self.velocities[i] = self.rho*self.velocities[i] + (1-self.rho)*self.parameters[i]['grad']\n",
    "            self.parameters[i]['val'] -= (self.lr*self.velocities[i] + self.reg*self.parameters[i]['val'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "(1) Prepare Data: Load, Shuffle, Normalization, Batching, Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "#mnist.init()\n",
    "X_train, Y_train, X_test, Y_test = load()\n",
    "X_train, X_test = X_train/float(255), X_test/float(255)\n",
    "X_train -= np.mean(X_train)\n",
    "X_test -= np.mean(X_test)\n",
    "\n",
    "batch_size = 16\n",
    "D_in = 784\n",
    "D_out = 10\n",
    "\n",
    "print(\"batch_size: \" + str(batch_size) + \", D_in: \" + str(D_in) + \", D_out: \" + str(D_out))\n",
    "\n",
    "### TWO LAYER NET FORWARD TEST ###\n",
    "#H=400\n",
    "#model = nn.TwoLayerNet(batch_size, D_in, H, D_out)\n",
    "H1=300\n",
    "H2=100\n",
    "# model = ThreeLayerNet(batch_size, D_in, H1, H2, D_out)\n",
    "model = LeNet5()\n",
    "\n",
    "\n",
    "losses = []\n",
    "#optim = optimizer.SGD(model.get_params(), lr=0.0001, reg=0)\n",
    "optim = SGDMomentum(model.get_params(), lr=0.0001, momentum=0.80, reg=0.00003)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# TRAIN\n",
    "ITER = 25000\n",
    "for i in range(ITER):\n",
    "    # get batch, make onehot\n",
    "    X_batch, Y_batch = get_batch(X_train, Y_train, batch_size)\n",
    "    Y_batch = MakeOneHot(Y_batch, D_out)\n",
    "    \n",
    "    # forward, loss, backward, step\n",
    "    X_batch = X_batch.reshape(batch_size,1,28,28)\n",
    "    Y_pred = model.forward(X_batch)\n",
    "    loss, dout = criterion.get(Y_pred, Y_batch)\n",
    "    model.backward(dout)\n",
    "    optim.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"%s%% iter: %s, loss: %s\" % (100*i/ITER,i, loss))\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "# save params\n",
    "weights = model.get_params()\n",
    "with open(\"weights.pkl\",\"wb\") as f:\n",
    "    pickle.dump(weights, f)\n",
    "\n",
    "draw_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET ACC\n",
    "X_train = X_train.reshape(len(X_train),1,28,28)\n",
    "Y_pred = model.forward(X_train)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_train\n",
    "result = list(result)\n",
    "print(\"TRAIN--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_train.shape[0]) + \", acc=\" + str(result.count(0)/X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET ACC\n",
    "X_test = X_test.reshape(len(X_test),1,28,28)\n",
    "Y_pred = model.forward(X_test)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_test\n",
    "result = list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST--> Correct: 9588 out of 10000, acc=0.9588\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_test.shape[0]) + \", acc=\" + str(result.count(0)/X_test.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

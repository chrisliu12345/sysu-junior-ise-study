报告内容主要包括三部分：
contribution: 阐述论文中主要的贡献，如提出的新方法，或在前人的模型中如何做出改进，了解论文之前的 模型存在那些不足，当前模型如何做出改进。 

method: 详细描述论文中提出的新模型或新方法，可细化到每个公式，理解作者的行文思路，了解算法背后 的实现原理。

advantages and disadvantages：通读论文后，总结文中提出的方法的优缺点，解读方法中的重要步骤， 并分析可能存在的问题。

## BiRNN

### contribution

在这篇论文之前，对于处理不定长的时间序列的数据使用的方法主要有TDNN和RNN，作者在各种问题上的实验表明RNN在处理这些数据上有着更大的优势，但传统的单向RNN只能获取到`left-to-right`的单向信息，作者通过加入反向的RNN，使得网络可以同时获取到双向的信息，进而提高模型的准确度。

如下图，传统的MLP只适用于固定长度的数据，TDNN通过设定一个窗口，截取一定长度的数据，一定程度上解决了不定长数据的问题，但网络获取的信息受限于窗口的大小，RNN则通过`state`的传递，理论上可以获取到整个序列的信息，但实际上RNN对于长序列的处理并不好，通过加入`delay`一定程度上可以改善模型，但这一问题依然存在，因此作者通过结果双向的RNN尝试解决这一问题

![image-20191102160849785](期中报告.assets/image-20191102160849785.png)

### method

传统的RNN如下图所示

![image-20191102164147766](期中报告.assets/image-20191102164147766.png)

通过加入反向的RNN，模型如下图所示

![image-20191101115250173](期中报告.assets/image-20191101115250173.png)

`t`时段的输入由`Forward States`和`Backward States`的权重处理后得到两个结果，两个结果经过线性变化（可通过训练得出相应的权重）得到`t`时段的输出，前向与反向的`state`是单向传递的，相互之间没有连接

训练时，先后正反向计算RNN State，然后根据线性变化得到output

反向传播则先对output求导，然后先后对正反向求导，最后更新权重

### advantages and disadvantages

由于RNN在梯度传播中的梯度消失/爆炸，RNN只能获取到短的时间关系，BiRNN通过加入反向RNN的方法虽然让网络得以利用反向的信息，但并没有解决RNN无法获取长序列信息的问题

## LSTM

### contribution

针对RNN无法获取长序列信息的问题，LSTM通过改进RNN网络的结构，通过

### method

### advantages and disadvantages

## GRU

### contribution

### method

### advantages and disadvantages

## seq2seq

### contribution

### method

### advantages and disadvantages

## Transformer

### contribution

### method

### advantages and disadvantages

[blog](https://nonexistent.tech/2019/01/22/Attention Is All You Need/) 

##  

## Attention

### contribution

### method

### advantages and disadvantages

## Soft Attention

### contribution

### method

### advantages and disadvantages


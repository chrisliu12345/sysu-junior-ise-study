{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理参考https://github.com/becxer/cnn-dailymail/ ，对源代码做了修改让代码输出txt文件，相应代码在./data/cnn-dailymail中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标点符号去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\"-lrb-\", \"-rrb-\", \"-\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/cnn-dailymail/finished_files/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"'!\\\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'.\" \n",
    "\n",
    "def get_xy_tuple(content, summary):\n",
    "    x = read_content(content)\n",
    "    y = read_summary(summary)\n",
    "\n",
    "    if x != None and y != None:\n",
    "        return (x, y)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def read_summary(summary):\n",
    "    summary = summary.replace(\"<s>\", \"\")\n",
    "    summary = summary.replace(\"</s>\", \"\")\n",
    "    summary = remove_punctuations(summary)\n",
    "#     summary = summary.split()\n",
    "#     words = [word for word in summary if word not in stop_words]\n",
    "    return summary\n",
    "    \n",
    "def read_content(content):\n",
    "    content = remove_punctuations(content)\n",
    "#     content = content.split()\n",
    "#     words = [word for word in content if word not in stop_words]\n",
    "    return content\n",
    "\n",
    "def remove_punctuations(doc):\n",
    "    if isinstance(doc, str):\n",
    "        return doc.translate(str.maketrans('', '', punctuation))\n",
    "    elif isinstance(doc, list):\n",
    "        return [line.translate(str.maketrans('', '', punctuation)) for line in doc]\n",
    "\n",
    "lines = []\n",
    "x = []\n",
    "with open(data_dir) as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\"\\n\")\n",
    "        if line.startswith(\"<s>\"):\n",
    "            y = line\n",
    "        elif line.endswith(\"</content>\"):\n",
    "            xy_tuple = get_xy_tuple(x,y)\n",
    "            x = []\n",
    "            if xy_tuple != None:\n",
    "                lines.append(xy_tuple)\n",
    "        elif line.startswith(\"<content>\"):\n",
    "            x.append(line[9:])\n",
    "        else:\n",
    "            x.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取句向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过word2vec平均或者用tfidf加权平均的方式得到句向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec模型使用谷歌在GoogleNews上训练的模型，可见https://code.google.com/archive/p/word2vec/\n",
    "下载地址：https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "    './data/GoogleNews-vectors-negative300.bin', binary=True, limit=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 57857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "a = [\"\".join(line[0]) for line in lines]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "X = vectorizer.fit_transform(a)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentences):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        vec = np.zeros((300,))\n",
    "        num_word = 0\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words and word in word2vec:\n",
    "                num_word += 1\n",
    "                vec += word2vec[word]\n",
    "        if num_word:\n",
    "            res.append(vec/num_word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfVec(sentences, tfidf):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        vec = np.zeros((300,))\n",
    "        num_word = 0\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words and word in word2vec and word in tfidf:\n",
    "                num_word += 1\n",
    "                vec += word2vec[word] * tfidf[word]\n",
    "        if num_word:\n",
    "            res.append(vec/num_word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InferSent使用的词嵌入和训练所得的模型参数下载地址如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove：http://nlp.stanford.edu/data/glove.840B.300d.zip \n",
    "\n",
    "fastText：https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip \n",
    "\n",
    "Glove model：https://dl.fbaipublicfiles.com/infersent/infersent1.pkl \n",
    "\n",
    "fastText model：https://dl.fbaipublicfiles.com/infersent/infersent2.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
    "                                bidirectional=True, dropout=self.dpout_model)\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # either all weights are on cpu or they are on gpu\n",
    "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len: [max_len, ..., min_len] (bsize)\n",
    "        # sent: (seqlen x bsize x worddim)\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # Sort by length (keep idx)\n",
    "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        sent_len_sorted = sent_len_sorted.copy()\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "\n",
    "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_sort)\n",
    "        sent = sent.index_select(1, idx_sort)\n",
    "\n",
    "        # Handling padding in Recurrent Networks\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
    "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
    "\n",
    "        # Un-sort by length\n",
    "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_unsort)\n",
    "        sent_output = sent_output.index_select(1, idx_unsort)\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
    "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
    "            emb = emb / sent_len.expand_as(emb)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb = torch.max(sent_output, 0)[0]\n",
    "            if emb.ndimension() == 3:\n",
    "                emb = emb.squeeze(0)\n",
    "                assert emb.ndimension() == 2\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with w2v vectors\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with k first w2v vectors\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if k <= K:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "                    k += 1\n",
    "                if k > K:\n",
    "                    if word in [self.bos, self.eos]:\n",
    "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    # build w2v vocab with k most frequent words\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (K))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # keep only new words\n",
    "        for word in self.word_vec:\n",
    "            if word in word_dict:\n",
    "                del word_dict[word]\n",
    "\n",
    "        # udpate vocabulary\n",
    "        if word_dict:\n",
    "            new_word_vec = self.get_w2v(word_dict)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "        else:\n",
    "            new_word_vec = []\n",
    "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # sent in batch in decreasing order of lengths\n",
    "        # batch: (bsize, max_len, word_dim)\n",
    "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "        return torch.FloatTensor(embed)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        if self.moses_tok:\n",
    "            s = ' '.join(word_tokenize(s))\n",
    "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
    "            return s.split()\n",
    "        else:\n",
    "            return word_tokenize(s)\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
    "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # filters words without w2v vectors\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
    "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
    "                s_f = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
    "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
    "\n",
    "        # sort by decreasing length\n",
    "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "        sentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "        return sentences, lengths, idx_sort\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "        sentences, lengths, idx_sort = self.prepare_samples(\n",
    "                        sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences), bsize):\n",
    "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            with torch.no_grad():\n",
    "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "            embeddings.append(batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # unsort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
    "\n",
    "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
    "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        batch = self.get_batch(sent)\n",
    "\n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.enc_lstm(batch)[0]\n",
    "        output, idxs = torch.max(output, 0)\n",
    "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
    "        idxs = idxs.data.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "        # visualize model\n",
    "        import matplotlib.pyplot as plt\n",
    "        x = range(len(sent[0]))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent[0], rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "print(\"model_version:\",model_version)\n",
    "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = True\n",
    "model = model.cuda() if use_cuda else model\n",
    "\n",
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = '../data/glove.840B.300d.txt' if model_version == 1 else '../data/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "# Load embeddings of K most frequent words\n",
    "model.build_vocab_k_words(K=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107523113d684265b464f24a25c9e8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: ['lrb cnn rrb once hillary clinton s official announcement went online  social media responded in a big way  with terms like  hillary clinton    hillary2016   and yes  even  whyimnotvotingforhillary  trending ', 'certainly  you could nt go far on twitter lrb even before clinton tweeted her announcement rrb  without an opinion or thought on her new campaign lrb there were over 3 million views of her announcment tweets in one hour  and 750000 facebook video views so far by sunday evening rrb ']  response across social media led to multiple trending topics for hillary clinton s presidential announcement    some responded to her video and her new campaign logo  \n",
      "error: ['the masters 2015 is almost here  to help get you in the mood for the first major of the year  rory mcilroy  ian poulter  graeme mcdowell and justin rose  plus past masters champions nick faldo and charl schwartzel  give the lowdown on every hole at the worldfamous augusta national golf club ']  the 79th masters tournament gets underway at augusta national on thursday    rory mcilroy and tiger woods will be the star attractions in the field bidding for the green jacket at 2015 masters    mcilroy  justin rose  ian poulter  graeme mcdowell and more gave sportsmail the verdict on each hole at augusta    click on the brilliant interactive graphic below for details on each hole of the masters 2015 course    click here for all the latest news from the masters 2015  \n",
      "error: ['the buildup for the blockbuster fight between floyd mayweather and manny pacquiao in las vegas on may 2 steps up a gear on tuesday night when the american holds an open workout for the media ']  floyd mayweather holds an open media workout from 12 am uk lrb 7 pm edt rrb    the american takes on manny pacquiao in las vegas on may 2    mayweather s training is being streamed live across the world  \n",
      "error: ['twenty four hours after floyd mayweather jnr dazzled the media with his skills  manny pacquiao takes his turn ', 'the filipino icon will be put through his paces at the iconic wild card gym in los angeles this evening under the watchful eye of trainer freddie roach ']  manny pacquiao takes on floyd mayweather in las vegas on may 2    pacquiao will hold a public workout tonight in his gym in la    mayweather held his last night at the mayweather boxing gym    the fight will be worth at least  300m  the richest in boxing history    read  mayweather admits he no longer enjoys boxing  \n",
      "error: ['sportsmail s boxing correspondent jeff powell looks ahead to saturday s megafight at the mgm grand after witnessing floyd mayweather and manny pacquiao s grand arrivals in las vegas ', 'both boxers made public appearances on tuesday as their  300million showdown draws ever closer  and our man powell was there ']  jeff powell looks ahead to saturday s fight at the mgm grand    floyd mayweather takes on manny pacquiao in  300m showdown    both fighters arrived in las vegas on tuesday with public appearances    read  mayweather makes official arrival ahead of manny pacquiao fight    al haymon  the man behind mayweather who is revolutionising boxing    mayweather vs pacquiao takes centre stage  but who s on the undercard  \n",
      "error: []  scott dann was a fraction offside when he set up glenn murray    assistant john brooks was spot on with two close calls in same move    crystal palace beat manchester city 21 in premier league on monday  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypothesis = []\n",
    "reference = []\n",
    "n_clusters = 3\n",
    "\n",
    "for x,y in tqdm_notebook(lines):\n",
    "    try:\n",
    "        sentVec = sentence2vec(x)\n",
    "#         sentVec = tfidfVec(x, tfidf)\n",
    "#         sentVec = model.encode(x)\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        kmeans = kmeans.fit(sentVec)\n",
    "        avg = []\n",
    "        for j in range(n_clusters):\n",
    "            idx = np.where(kmeans.labels_ == j)[0]\n",
    "            avg.append(np.mean(idx))\n",
    "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, sentVec)\n",
    "        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "        summary = ' '.join([x[closest[idx]] for idx in ordering])\n",
    "\n",
    "        reference.append(y)\n",
    "        hypothesis.append(summary)\n",
    "    except:\n",
    "        print(\"error:\", x, y)\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化聚类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16e00509c48>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "\n",
    "x = lines[0][0]\n",
    "y = lines[0][1]\n",
    "\n",
    "rouge = Rouge()\n",
    "c = list(itertools.combinations(x,3))\n",
    "max_scores = 0\n",
    "for sents in tqdm_notebook(c):\n",
    "    hypothesis = \"\".join(sents)\n",
    "    score = rouge.get_scores(hypothesis, y)[0]\n",
    "    score = (score[\"rouge-1\"][\"f\"] + score[\"rouge-2\"][\"f\"] + score[\"rouge-l\"][\"f\"]) / 3\n",
    "    if score > max_scores:\n",
    "        max_scores = score\n",
    "        best_c = sent\n",
    "\n",
    "sentVec = sentence2vec(x)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(sentVec)\n",
    "a = pca.transform(sentVec)\n",
    "plt.scatter(a[:3,0], a[:3,1], marker = 'x', color = 'm', label='first3')\n",
    "plt.scatter(a[3:,0], a[3:,1])\n",
    "\n",
    "plt.scatter(a[best3_idx[0],0], a[best3_idx[0],1], marker='x', color = 'y')\n",
    "for i in best3_idx:\n",
    "    plt.scatter(a[i,0], a[i,1], marker='x', color = 'y', label='best3')\n",
    "\n",
    "kmeans = KMeans(n_clusters=3,random_state=42)\n",
    "kmeans = kmeans.fit(sentVec)\n",
    "centers = pca.transform(kmeans.cluster_centers_)\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, sentVec)\n",
    "for c in closest:\n",
    "    plt.scatter(a[c,0], a[c,1], color='g', label=\"kmeans\")\n",
    "plt.scatter(centers[:,0], centers[:,1], marker = \"o\", color = 'r', label='center')\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "rouge = Rouge()\n",
    "\n",
    "hypothesis = []\n",
    "reference = []\n",
    "sn = 3\n",
    "\n",
    "for x,y in tqdm_notebook(lines):\n",
    "    try:\n",
    "        sentence_vectors = sentence2vec(x)\n",
    "#         sentence_vectors = tfidfVec(x, tfidf)\n",
    "#         sentence_vectors = model.encode(x)\n",
    "        sim_mat = np.zeros([len(x), len(x)])\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                if i != j:\n",
    "                    sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        rank_scores = sorted(range(len(scores)), key=lambda k: scores.get(k), reverse=True)\n",
    "        ordering = sorted(range(sn), key=lambda k: rank_scores[k])\n",
    "\n",
    "        hypothesis.append(''.join([x[rank_scores[idx]] for idx in ordering]))\n",
    "        reference.append(y)\n",
    "    except:\n",
    "        print(\"error:\",x,y)\n",
    "        continue\n",
    "\n",
    "    \n",
    "scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def calculateSimilarity(sentence,doc):\n",
    "    cos_sim = 0.0\n",
    "    for vec in doc:\n",
    "        try:\n",
    "            cos_sim += np.inner(sentence, vec) / (np.linalg.norm(sentence) * np.linalg.norm(vec))\n",
    "        except:\n",
    "            continue\n",
    "    return cos_sim / len(doc)\n",
    "    \n",
    "alpha = 0.5\n",
    "print(\"alpha:\", alpha)\n",
    "for x,y in tqdm(lines):\n",
    "    try:\n",
    "        doc = [\"\".join(x)]\n",
    "#         x_vec = sentence2vec(x)\n",
    "#         doc_vec = sentence2vec(doc)\n",
    "        \n",
    "#         x_vec = tfidfVec(x, tfidf)\n",
    "#         doc_vec =  tfidfVec(doc, tfidf)\n",
    "        x_vec = model.encode(x)\n",
    "        doc_vec =  model.encode(doc)\n",
    "\n",
    "        scores = []\n",
    "        for vec in x_vec:\n",
    "            scores.append(calculateSimilarity(vec, doc_vec))\n",
    "\n",
    "        max_index = scores.index(max(scores))\n",
    "\n",
    "        n = 3\n",
    "        summarySet = []\n",
    "\n",
    "        while n > 0:\n",
    "            mmr = {}\n",
    "            for index in range(len(x)):\n",
    "                sentence = x[index]\n",
    "                vec = x_vec[index]\n",
    "                if not sentence in summarySet:\n",
    "                    mmr[sentence] = alpha * scores[index] - (1 - alpha) * calculateSimilarity(vec, doc_vec)\n",
    "            selected = max(mmr.items(), key=operator.itemgetter(1))[0]\n",
    "            summarySet.append(selected)\n",
    "            n -= 1\n",
    "        hypothesis.append(\"\".join(summarySet))\n",
    "        reference.append(y)\n",
    "    except Exception as e:\n",
    "        print(\"error:\", x, y)\n",
    "        print(e)\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接取前三句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e3ac6923004dbabd199c4aa5246bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.4061975886142278,\n",
       "  'p': 0.3492116467886355,\n",
       "  'r': 0.5203605937110696},\n",
       " 'rouge-2': {'f': 0.17052685675218657,\n",
       "  'p': 0.14371171733197152,\n",
       "  'r': 0.23018089464057517},\n",
       " 'rouge-l': {'f': 0.24860135480039966,\n",
       "  'p': 0.23403361999142586,\n",
       "  'r': 0.35053142615024646}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "hypothesis = []\n",
    "reference = []\n",
    "\n",
    "for x,y in tqdm_notebook(lines):\n",
    "    first3sentence = x[:3]\n",
    "    if len(\"\".join(first3sentence)) < 10:\n",
    "        continue\n",
    "    hypothesis.append(''.join(first3sentence))\n",
    "    reference.append(y)\n",
    "\n",
    "    \n",
    "scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机取三句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b13c20aeb34fc588031cb71e64eddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.2890532563335098,\n",
       "  'p': 0.2583631099718259,\n",
       "  'r': 0.3578374087947165},\n",
       " 'rouge-2': {'f': 0.08516945177073726,\n",
       "  'p': 0.07553395151286886,\n",
       "  'r': 0.1098537965184604},\n",
       " 'rouge-l': {'f': 0.173403734703439,\n",
       "  'p': 0.1686255879027422,\n",
       "  'r': 0.23433493594892973}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "rouge = Rouge()\n",
    "hypothesis = []\n",
    "reference = []\n",
    "\n",
    "for x,y in tqdm_notebook(lines):\n",
    "    c = list(itertools.combinations(x,3))\n",
    "    try:\n",
    "        h = random.choice(c)\n",
    "    except:\n",
    "        continue\n",
    "    if len(\"\".join(h)) < 10:\n",
    "        continue\n",
    "    hypothesis.append(\"\".join(h))\n",
    "    reference.append(y)\n",
    "\n",
    "\n",
    "scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理论最优三句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "hypothesis = []\n",
    "reference = []\n",
    "\n",
    "for x,y in tqdm_notebook(lines):\n",
    "    try:\n",
    "        max_scores = 0\n",
    "        c = list(itertools.combinations(x,3))\n",
    "        for a in c:\n",
    "            scores = rouge.get_scores(\"\".join(a), y)[0]\n",
    "            avg_scores = (scores[\"rouge-1\"][\"f\"] +  scores[\"rouge-1\"][\"f\"] + scores[\"rouge-1\"][\"f\"]) / 3\n",
    "            if avg_scores > max_scores:\n",
    "                max_scores = avg_scores\n",
    "                best_sents = \"\".join(a)\n",
    "\n",
    "        hypothesis.append(best_sents)\n",
    "        reference.append(y)\n",
    "    except:\n",
    "        print(\"error:\",x,y)\n",
    "\n",
    "\n",
    "scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

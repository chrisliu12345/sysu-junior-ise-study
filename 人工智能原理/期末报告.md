[Automatic text summarization: What has been done and what has to be done](https://arxiv.org/abs/1904.00688) 

大致日程

- 20日前完成基础框架
- 26日前完成论文总结

[A trainable document summarizer](https://dl.acm.org/citation.cfm?doid=215206.215333)

[Discovery of topically coherent sentences for extractive summarization](https://dl.acm.org/citation.cfm?id=2002472.2002535)



### TextRank



### A Neural Attention Model for Abstractive Sentence Summarization

论文：https://arxiv.org/abs/1509.00685 

代码：https://github.com/facebookarchive/NAMAS 

attention+seq2seq



### Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond

论文：https://arxiv.org/abs/1602.06023

attention+seq2seq

“Large Vocabulary Trick” (LVT)



### On Using Very Large Target Vocabulary for Neural Machine Translation

论文：https://arxiv.org/abs/1412.2007 



### Coarse-to-Fine Attention Models for Document Summarization

论文：https://www.aclweb.org/anthology/W17-4505.pdf

hard attention+seq2seq



### Extraction of Salient Sentences from Labelled Documents

https://arxiv.org/abs/1412.6815 

CNN



### Fine-tune BERT for Extractive Summarization 

https://arxiv.org/abs/1903.10318 



## 模型评价方法

Rouge指标
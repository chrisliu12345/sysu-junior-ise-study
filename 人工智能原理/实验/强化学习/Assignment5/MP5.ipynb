{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "e:0.0  memory length:62178  epsilon:1.0   steps:126    evaluation reward:1.16\nepisode:346  score:1.0  memory length:62354  epsilon:1.0   steps:176    evaluation reward:1.17\nepisode:347  score:0.0  memory length:62483  epsilon:1.0   steps:129    evaluation reward:1.13\nepisode:348  score:2.0  memory length:62676  epsilon:1.0   steps:193    evaluation reward:1.15\nepisode:349  score:0.0  memory length:62803  epsilon:1.0   steps:127    evaluation reward:1.15\nepisode:350  score:0.0  memory length:62939  epsilon:1.0   steps:136    evaluation reward:1.11\nepisode:351  score:3.0  memory length:63188  epsilon:1.0   steps:249    evaluation reward:1.13\nepisode:352  score:1.0  memory length:63361  epsilon:1.0   steps:173    evaluation reward:1.11\nepisode:353  score:0.0  memory length:63491  epsilon:1.0   steps:130    evaluation reward:1.08\nepisode:354  score:2.0  memory length:63677  epsilon:1.0   steps:186    evaluation reward:1.06\nepisode:355  score:0.0  memory length:63807  epsilon:1.0   steps:130    evaluation reward:1.06\nepisode:356  score:4.0  memory length:64111  epsilon:1.0   steps:304    evaluation reward:1.08\nepisode:357  score:1.0  memory length:64286  epsilon:1.0   steps:175    evaluation reward:1.08\nepisode:358  score:2.0  memory length:64505  epsilon:1.0   steps:219    evaluation reward:1.1\nepisode:359  score:2.0  memory length:64712  epsilon:1.0   steps:207    evaluation reward:1.11\nepisode:360  score:2.0  memory length:64908  epsilon:1.0   steps:196    evaluation reward:1.11\nepisode:361  score:1.0  memory length:65084  epsilon:1.0   steps:176    evaluation reward:1.11\nepisode:362  score:2.0  memory length:65283  epsilon:1.0   steps:199    evaluation reward:1.12\nepisode:363  score:2.0  memory length:65507  epsilon:1.0   steps:224    evaluation reward:1.14\nepisode:364  score:4.0  memory length:65809  epsilon:1.0   steps:302    evaluation reward:1.15\nepisode:365  score:1.0  memory length:65967  epsilon:1.0   steps:158    evaluation reward:1.14\nepisode:366  score:1.0  memory length:66137  epsilon:1.0   steps:170    evaluation reward:1.15\nepisode:367  score:4.0  memory length:66405  epsilon:1.0   steps:268    evaluation reward:1.19\nepisode:368  score:2.0  memory length:66629  epsilon:1.0   steps:224    evaluation reward:1.21\nepisode:369  score:1.0  memory length:66806  epsilon:1.0   steps:177    evaluation reward:1.21\nepisode:370  score:1.0  memory length:66958  epsilon:1.0   steps:152    evaluation reward:1.2\nepisode:371  score:1.0  memory length:67146  epsilon:1.0   steps:188    evaluation reward:1.21\nepisode:372  score:0.0  memory length:67285  epsilon:1.0   steps:139    evaluation reward:1.18\nepisode:373  score:1.0  memory length:67449  epsilon:1.0   steps:164    evaluation reward:1.19\nepisode:374  score:1.0  memory length:67634  epsilon:1.0   steps:185    evaluation reward:1.18\nepisode:375  score:1.0  memory length:67791  epsilon:1.0   steps:157    evaluation reward:1.19\nepisode:376  score:2.0  memory length:68000  epsilon:1.0   steps:209    evaluation reward:1.21\nepisode:377  score:3.0  memory length:68253  epsilon:1.0   steps:253    evaluation reward:1.24\nepisode:378  score:2.0  memory length:68448  epsilon:1.0   steps:195    evaluation reward:1.25\nepisode:379  score:3.0  memory length:68680  epsilon:1.0   steps:232    evaluation reward:1.27\nepisode:380  score:1.0  memory length:68856  epsilon:1.0   steps:176    evaluation reward:1.28\nepisode:381  score:0.0  memory length:68985  epsilon:1.0   steps:129    evaluation reward:1.28\nepisode:382  score:2.0  memory length:69207  epsilon:1.0   steps:222    evaluation reward:1.29\nepisode:383  score:2.0  memory length:69408  epsilon:1.0   steps:201    evaluation reward:1.3\nepisode:384  score:1.0  memory length:69587  epsilon:1.0   steps:179    evaluation reward:1.31\nepisode:385  score:0.0  memory length:69715  epsilon:1.0   steps:128    evaluation reward:1.29\nepisode:386  score:1.0  memory length:69895  epsilon:1.0   steps:180    evaluation reward:1.3\nepisode:387  score:1.0  memory length:70053  epsilon:1.0   steps:158    evaluation reward:1.3\nepisode:388  score:1.0  memory length:70229  epsilon:1.0   steps:176    evaluation reward:1.3\nepisode:389  score:1.0  memory length:70404  epsilon:1.0   steps:175    evaluation reward:1.29\nepisode:390  score:1.0  memory length:70573  epsilon:1.0   steps:169    evaluation reward:1.3\nepisode:391  score:1.0  memory length:70757  epsilon:1.0   steps:184    evaluation reward:1.3\nepisode:392  score:0.0  memory length:70888  epsilon:1.0   steps:131    evaluation reward:1.29\nepisode:393  score:2.0  memory length:71109  epsilon:1.0   steps:221    evaluation reward:1.3\nepisode:394  score:2.0  memory length:71325  epsilon:1.0   steps:216    evaluation reward:1.3\nepisode:395  score:3.0  memory length:71565  epsilon:1.0   steps:240    evaluation reward:1.31\nepisode:396  score:1.0  memory length:71751  epsilon:1.0   steps:186    evaluation reward:1.3\nepisode:397  score:0.0  memory length:71879  epsilon:1.0   steps:128    evaluation reward:1.3\nepisode:398  score:1.0  memory length:72063  epsilon:1.0   steps:184    evaluation reward:1.3\nepisode:399  score:0.0  memory length:72198  epsilon:1.0   steps:135    evaluation reward:1.3\nepisode:400  score:1.0  memory length:72382  epsilon:1.0   steps:184    evaluation reward:1.3\nepisode:401  score:2.0  memory length:72599  epsilon:1.0   steps:217    evaluation reward:1.27\nepisode:402  score:1.0  memory length:72781  epsilon:1.0   steps:182    evaluation reward:1.28\nepisode:403  score:1.0  memory length:72953  epsilon:1.0   steps:172    evaluation reward:1.28\nepisode:404  score:4.0  memory length:73216  epsilon:1.0   steps:263    evaluation reward:1.3\nepisode:405  score:0.0  memory length:73353  epsilon:1.0   steps:137    evaluation reward:1.28\nepisode:406  score:1.0  memory length:73509  epsilon:1.0   steps:156    evaluation reward:1.26\nepisode:407  score:1.0  memory length:73663  epsilon:1.0   steps:154    evaluation reward:1.25\nepisode:408  score:2.0  memory length:73862  epsilon:1.0   steps:199    evaluation reward:1.27\nepisode:409  score:1.0  memory length:74021  epsilon:1.0   steps:159    evaluation reward:1.27\nepisode:410  score:0.0  memory length:74160  epsilon:1.0   steps:139    evaluation reward:1.27\nepisode:411  score:0.0  memory length:74282  epsilon:1.0   steps:122    evaluation reward:1.25\nepisode:412  score:0.0  memory length:74417  epsilon:1.0   steps:135    evaluation reward:1.23\nepisode:413  score:0.0  memory length:74560  epsilon:1.0   steps:143    evaluation reward:1.23\nepisode:414  score:3.0  memory length:74792  epsilon:1.0   steps:232    evaluation reward:1.24\nepisode:415  score:1.0  memory length:74966  epsilon:1.0   steps:174    evaluation reward:1.24\nepisode:416  score:2.0  memory length:75165  epsilon:1.0   steps:199    evaluation reward:1.22\nepisode:417  score:1.0  memory length:75346  epsilon:1.0   steps:181    evaluation reward:1.23\nepisode:418  score:1.0  memory length:75506  epsilon:1.0   steps:160    evaluation reward:1.23\nepisode:419  score:1.0  memory length:75685  epsilon:1.0   steps:179    evaluation reward:1.24\nepisode:420  score:1.0  memory length:75846  epsilon:1.0   steps:161    evaluation reward:1.24\nepisode:421  score:6.0  memory length:76166  epsilon:1.0   steps:320    evaluation reward:1.27\nepisode:422  score:2.0  memory length:76386  epsilon:1.0   steps:220    evaluation reward:1.28\nepisode:423  score:4.0  memory length:76644  epsilon:1.0   steps:258    evaluation reward:1.32\nepisode:424  score:3.0  memory length:76906  epsilon:1.0   steps:262    evaluation reward:1.35\nepisode:425  score:2.0  memory length:77126  epsilon:1.0   steps:220    evaluation reward:1.36\nepisode:426  score:0.0  memory length:77253  epsilon:1.0   steps:127    evaluation reward:1.32\nepisode:427  score:1.0  memory length:77429  epsilon:1.0   steps:176    evaluation reward:1.33\nepisode:428  score:0.0  memory length:77563  epsilon:1.0   steps:134    evaluation reward:1.32\nepisode:429  score:0.0  memory length:77698  epsilon:1.0   steps:135    evaluation reward:1.31\nepisode:430  score:1.0  memory length:77872  epsilon:1.0   steps:174    evaluation reward:1.32\nepisode:431  score:1.0  memory length:78032  epsilon:1.0   steps:160    evaluation reward:1.33\nepisode:432  score:2.0  memory length:78272  epsilon:1.0   steps:240    evaluation reward:1.35\nepisode:433  score:1.0  memory length:78433  epsilon:1.0   steps:161    evaluation reward:1.35\nepisode:434  score:0.0  memory length:78573  epsilon:1.0   steps:140    evaluation reward:1.33\nepisode:435  score:0.0  memory length:78707  epsilon:1.0   steps:134    evaluation reward:1.33\nepisode:436  score:1.0  memory length:78869  epsilon:1.0   steps:162    evaluation reward:1.33\nepisode:437  score:2.0  memory length:79071  epsilon:1.0   steps:202    evaluation reward:1.34\nepisode:438  score:2.0  memory length:79255  epsilon:1.0   steps:184    evaluation reward:1.36\nepisode:439  score:1.0  memory length:79428  epsilon:1.0   steps:173    evaluation reward:1.36\nepisode:440  score:2.0  memory length:79625  epsilon:1.0   steps:197    evaluation reward:1.36\nepisode:441  score:0.0  memory length:79761  epsilon:1.0   steps:136    evaluation reward:1.32\nepisode:442  score:0.0  memory length:79888  epsilon:1.0   steps:127    evaluation reward:1.3\nepisode:443  score:1.0  memory length:80047  epsilon:1.0   steps:159    evaluation reward:1.31\nepisode:444  score:5.0  memory length:80344  epsilon:1.0   steps:297    evaluation reward:1.35\nepisode:445  score:1.0  memory length:80523  epsilon:1.0   steps:179    evaluation reward:1.36\nepisode:446  score:2.0  memory length:80728  epsilon:1.0   steps:205    evaluation reward:1.37\nepisode:447  score:2.0  memory length:80949  epsilon:1.0   steps:221    evaluation reward:1.39\nepisode:448  score:0.0  memory length:81083  epsilon:1.0   steps:134    evaluation reward:1.37\nepisode:449  score:3.0  memory length:81366  epsilon:1.0   steps:283    evaluation reward:1.4\nepisode:450  score:1.0  memory length:81538  epsilon:1.0   steps:172    evaluation reward:1.41\nepisode:451  score:0.0  memory length:81670  epsilon:1.0   steps:132    evaluation reward:1.38\nepisode:452  score:1.0  memory length:81847  epsilon:1.0   steps:177    evaluation reward:1.38\nepisode:453  score:2.0  memory length:82047  epsilon:1.0   steps:200    evaluation reward:1.4\nepisode:454  score:4.0  memory length:82343  epsilon:1.0   steps:296    evaluation reward:1.42\nepisode:455  score:2.0  memory length:82577  epsilon:1.0   steps:234    evaluation reward:1.44\nepisode:456  score:1.0  memory length:82754  epsilon:1.0   steps:177    evaluation reward:1.41\nepisode:457  score:1.0  memory length:82928  epsilon:1.0   steps:174    evaluation reward:1.41\nepisode:458  score:0.0  memory length:83054  epsilon:1.0   steps:126    evaluation reward:1.39\nepisode:459  score:0.0  memory length:83186  epsilon:1.0   steps:132    evaluation reward:1.37\nepisode:460  score:1.0  memory length:83365  epsilon:1.0   steps:179    evaluation reward:1.36\nepisode:461  score:2.0  memory length:83588  epsilon:1.0   steps:223    evaluation reward:1.37\nepisode:462  score:1.0  memory length:83768  epsilon:1.0   steps:180    evaluation reward:1.36\nepisode:463  score:1.0  memory length:83927  epsilon:1.0   steps:159    evaluation reward:1.35\nepisode:464  score:0.0  memory length:84066  epsilon:1.0   steps:139    evaluation reward:1.31\nepisode:465  score:0.0  memory length:84191  epsilon:1.0   steps:125    evaluation reward:1.3\nepisode:466  score:2.0  memory length:84391  epsilon:1.0   steps:200    evaluation reward:1.31\nepisode:467  score:4.0  memory length:84652  epsilon:1.0   steps:261    evaluation reward:1.31\nepisode:468  score:0.0  memory length:84780  epsilon:1.0   steps:128    evaluation reward:1.29\nepisode:469  score:1.0  memory length:84951  epsilon:1.0   steps:171    evaluation reward:1.29\nepisode:470  score:3.0  memory length:85233  epsilon:1.0   steps:282    evaluation reward:1.31\nepisode:471  score:1.0  memory length:85409  epsilon:1.0   steps:176    evaluation reward:1.31\nepisode:472  score:3.0  memory length:85641  epsilon:1.0   steps:232    evaluation reward:1.34\nepisode:473  score:0.0  memory length:85772  epsilon:1.0   steps:131    evaluation reward:1.33\nepisode:474  score:2.0  memory length:85993  epsilon:1.0   steps:221    evaluation reward:1.34\nepisode:475  score:0.0  memory length:86119  epsilon:1.0   steps:126    evaluation reward:1.33\nepisode:476  score:2.0  memory length:86329  epsilon:1.0   steps:210    evaluation reward:1.33\nepisode:477  score:2.0  memory length:86537  epsilon:1.0   steps:208    evaluation reward:1.32\nepisode:478  score:1.0  memory length:86729  epsilon:1.0   steps:192    evaluation reward:1.31\nepisode:479  score:0.0  memory length:86862  epsilon:1.0   steps:133    evaluation reward:1.28\nepisode:480  score:1.0  memory length:87035  epsilon:1.0   steps:173    evaluation reward:1.28\nepisode:481  score:2.0  memory length:87255  epsilon:1.0   steps:220    evaluation reward:1.3\nepisode:482  score:1.0  memory length:87406  epsilon:1.0   steps:151    evaluation reward:1.29\nepisode:483  score:0.0  memory length:87540  epsilon:1.0   steps:134    evaluation reward:1.27\nepisode:484  score:3.0  memory length:87820  epsilon:1.0   steps:280    evaluation reward:1.29\nepisode:485  score:0.0  memory length:87953  epsilon:1.0   steps:133    evaluation reward:1.29\nepisode:486  score:2.0  memory length:88139  epsilon:1.0   steps:186    evaluation reward:1.3\nepisode:487  score:0.0  memory length:88268  epsilon:1.0   steps:129    evaluation reward:1.29\nepisode:488  score:1.0  memory length:88452  epsilon:1.0   steps:184    evaluation reward:1.29\nepisode:489  score:0.0  memory length:88580  epsilon:1.0   steps:128    evaluation reward:1.28\nepisode:490  score:1.0  memory length:88759  epsilon:1.0   steps:179    evaluation reward:1.28\nepisode:491  score:2.0  memory length:88974  epsilon:1.0   steps:215    evaluation reward:1.29\nepisode:492  score:1.0  memory length:89134  epsilon:1.0   steps:160    evaluation reward:1.3\nepisode:493  score:3.0  memory length:89402  epsilon:1.0   steps:268    evaluation reward:1.31\nepisode:494  score:0.0  memory length:89548  epsilon:1.0   steps:146    evaluation reward:1.29\nepisode:495  score:1.0  memory length:89721  epsilon:1.0   steps:173    evaluation reward:1.27\nepisode:496  score:1.0  memory length:89902  epsilon:1.0   steps:181    evaluation reward:1.27\nepisode:497  score:1.0  memory length:90081  epsilon:1.0   steps:179    evaluation reward:1.28\nepisode:498  score:0.0  memory length:90216  epsilon:1.0   steps:135    evaluation reward:1.27\nepisode:499  score:0.0  memory length:90345  epsilon:1.0   steps:129    evaluation reward:1.27\nepisode:500  score:1.0  memory length:90516  epsilon:1.0   steps:171    evaluation reward:1.27\nepisode:501  score:1.0  memory length:90707  epsilon:1.0   steps:191    evaluation reward:1.26\nepisode:502  score:2.0  memory length:90910  epsilon:1.0   steps:203    evaluation reward:1.27\nepisode:503  score:0.0  memory length:91044  epsilon:1.0   steps:134    evaluation reward:1.26\nepisode:504  score:0.0  memory length:91182  epsilon:1.0   steps:138    evaluation reward:1.22\nepisode:505  score:1.0  memory length:91352  epsilon:1.0   steps:170    evaluation reward:1.23\nepisode:506  score:4.0  memory length:91680  epsilon:1.0   steps:328    evaluation reward:1.26\nepisode:507  score:0.0  memory length:91811  epsilon:1.0   steps:131    evaluation reward:1.25\nepisode:508  score:1.0  memory length:91996  epsilon:1.0   steps:185    evaluation reward:1.24\nepisode:509  score:0.0  memory length:92130  epsilon:1.0   steps:134    evaluation reward:1.23\nepisode:510  score:0.0  memory length:92257  epsilon:1.0   steps:127    evaluation reward:1.23\nepisode:511  score:1.0  memory length:92418  epsilon:1.0   steps:161    evaluation reward:1.24\nepisode:512  score:0.0  memory length:92540  epsilon:1.0   steps:122    evaluation reward:1.24\nepisode:513  score:0.0  memory length:92663  epsilon:1.0   steps:123    evaluation reward:1.24\nepisode:514  score:1.0  memory length:92816  epsilon:1.0   steps:153    evaluation reward:1.22\nepisode:515  score:1.0  memory length:92982  epsilon:1.0   steps:166    evaluation reward:1.22\nepisode:516  score:0.0  memory length:93123  epsilon:1.0   steps:141    evaluation reward:1.2\nepisode:517  score:3.0  memory length:93365  epsilon:1.0   steps:242    evaluation reward:1.22\nepisode:518  score:2.0  memory length:93567  epsilon:1.0   steps:202    evaluation reward:1.23\nepisode:519  score:0.0  memory length:93692  epsilon:1.0   steps:125    evaluation reward:1.22\nepisode:520  score:1.0  memory length:93871  epsilon:1.0   steps:179    evaluation reward:1.22\nepisode:521  score:0.0  memory length:93994  epsilon:1.0   steps:123    evaluation reward:1.16\nepisode:522  score:1.0  memory length:94167  epsilon:1.0   steps:173    evaluation reward:1.15\nepisode:523  score:0.0  memory length:94293  epsilon:1.0   steps:126    evaluation reward:1.11\nepisode:524  score:0.0  memory length:94419  epsilon:1.0   steps:126    evaluation reward:1.08\nepisode:525  score:1.0  memory length:94576  epsilon:1.0   steps:157    evaluation reward:1.07\nepisode:526  score:2.0  memory length:94800  epsilon:1.0   steps:224    evaluation reward:1.09\nepisode:527  score:1.0  memory length:94976  epsilon:1.0   steps:176    evaluation reward:1.09\nepisode:528  score:1.0  memory length:95137  epsilon:1.0   steps:161    evaluation reward:1.1\nepisode:529  score:1.0  memory length:95326  epsilon:1.0   steps:189    evaluation reward:1.11\nepisode:530  score:1.0  memory length:95486  epsilon:1.0   steps:160    evaluation reward:1.11\nepisode:531  score:1.0  memory length:95643  epsilon:1.0   steps:157    evaluation reward:1.11\nepisode:532  score:0.0  memory length:95784  epsilon:1.0   steps:141    evaluation reward:1.09\nepisode:533  score:0.0  memory length:95913  epsilon:1.0   steps:129    evaluation reward:1.08\nepisode:534  score:0.0  memory length:96056  epsilon:1.0   steps:143    evaluation reward:1.08\nepisode:535  score:1.0  memory length:96234  epsilon:1.0   steps:178    evaluation reward:1.09\nepisode:536  score:2.0  memory length:96441  epsilon:1.0   steps:207    evaluation reward:1.1\nepisode:537  score:0.0  memory length:96583  epsilon:1.0   steps:142    evaluation reward:1.08\nepisode:538  score:0.0  memory length:96716  epsilon:1.0   steps:133    evaluation reward:1.06\nepisode:539  score:0.0  memory length:96852  epsilon:1.0   steps:136    evaluation reward:1.05\nepisode:540  score:2.0  memory length:97079  epsilon:1.0   steps:227    evaluation reward:1.05\nepisode:541  score:1.0  memory length:97251  epsilon:1.0   steps:172    evaluation reward:1.06\nepisode:542  score:1.0  memory length:97404  epsilon:1.0   steps:153    evaluation reward:1.07\nepisode:543  score:2.0  memory length:97632  epsilon:1.0   steps:228    evaluation reward:1.08\nepisode:544  score:0.0  memory length:97767  epsilon:1.0   steps:135    evaluation reward:1.03\nepisode:545  score:1.0  memory length:97945  epsilon:1.0   steps:178    evaluation reward:1.03\nepisode:546  score:1.0  memory length:98128  epsilon:1.0   steps:183    evaluation reward:1.02\nepisode:547  score:0.0  memory length:98254  epsilon:1.0   steps:126    evaluation reward:1.0\nepisode:548  score:3.0  memory length:98482  epsilon:1.0   steps:228    evaluation reward:1.03\nepisode:549  score:1.0  memory length:98638  epsilon:1.0   steps:156    evaluation reward:1.01\nepisode:550  score:1.0  memory length:98810  epsilon:1.0   steps:172    evaluation reward:1.01\nepisode:551  score:2.0  memory length:99034  epsilon:1.0   steps:224    evaluation reward:1.03\nepisode:552  score:3.0  memory length:99281  epsilon:1.0   steps:247    evaluation reward:1.05\nepisode:553  score:1.0  memory length:99467  epsilon:1.0   steps:186    evaluation reward:1.04\nepisode:554  score:0.0  memory length:99594  epsilon:1.0   steps:127    evaluation reward:1.0\nepisode:555  score:0.0  memory length:99726  epsilon:1.0   steps:132    evaluation reward:0.98\n"
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cf6731b38402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Start training after random sample generation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m# Update the target network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mUpdate_target_network_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yemq3\\Desktop\\学习\\人工智能原理\\实验\\强化学习\\Assignment5\\agent.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Compute Q(s_t, a) - Q of the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m### CODE ####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mstate_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# Compute Q function of next state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yemq3\\Desktop\\学习\\人工智能原理\\实验\\强化学习\\Assignment5\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 10:\n",
    "                torch.save(agent.model, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}